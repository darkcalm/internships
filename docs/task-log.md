2025-06-19T08:11:12Z - 1.1: Initialized project structure by creating data, deliverables, and docs directories. Files created: data/, deliverables/, docs/
2025-06-19T08:18:46Z - 1.2: Defined a JSON schema and created a sample personal data file. Files created: data/personal_data.json
2025-06-19T08:42:19Z - 1.3: Created the data parser module. Files created: util/data_parser.py
2025-06-19T08:54:11Z - 1.4: Implemented the function to load and validate personal data in the data parser module. Files modified: util/data_parser.py
2025-06-19T08:57:04Z - 1.5: Implemented the function to parse the job roster Markdown file. Files modified: util/data_parser.py
2025-06-20T07:52:26Z - 1.6: Implemented the function to parse the job submission HTML file. Files modified: util/data_parser.py
2025-06-20T14:26:15Z - 2.1: Created the letter generator module. Files created: util/letter_generator.py
2025-06-20T14:27:36Z - 2.2: Implemented the function to extract job requirements from parsed data. Files modified: util/letter_generator.py
2025-06-20T14:44:33Z - 2.3: Installed semantic similarity libraries (sentence-transformers, torch).
2025-06-20T14:45:12Z - 2.4: Implemented the semantic mapping function. Files modified: util/letter_generator.py
2025-06-20T14:46:30Z - 2.5: Created main.py and implemented initial data processing and mapping generation. Files created: main.py. Files modified: docs/task-log.md
2024-06-21T10:05:00Z - System Re-architecture: Pivoting to a new multi-stage, iterative AI-powered architecture based on user feedback. The previous linear script approach is now obsolete. The following files will be archived: main.py, util/data_parser.py, util/letter_generator.py, util/transformer.py, util/ui.py, tasks/tasks-prd-internship-assistant.md, data/personal_data.generated.json, data/mappings.json. User Prompt: "there should be steps where transformers can be used, and steps where the process is by script and no transformer is used: 1a. personal information sanitization and expansion. no transformers needed. sanitizing to perhaps a pdf then to a markdown, because in other cases might be provided not in html, but directly in pdf, or even directly in markdown (as you can see for other files under). an expansion process is needed because the user may provide more information at any point 1b. personal information word-by-word transformation. transformer may be applied. this step is for resolve formatting issues from pdf or markdown, to ensure that the word-by-word integrity of the input is kept within a structured json 2a. job description sanitization and expansion. no transformers needed. expansion is required because the first job description provided may be ambiguous, thus more information other than the job description is needed (either from minds for from credible sources) to fully utilize the potential connection between personal data and job decsription. the sanitization follows a similar workflow to 1a, allowing for user-provided html, pdf or markdown. the expansion is breadth-first, gathering new information either by user or through the model itself 2b. job description word-by-word transformation. transformer may be applied. this step is for resolve formatting issues from pdf or markdown, to ensure that the word-by-word integrity of the input is kept within a structured json 3. mapping. transformer may be applied. now we have one side that is provided by step 1, and another side provided by step 2. the mapping glues them together with either model-generated reasoning or by the user. the mapping is a structured json corresponding to the jsons from step 1 and 2. 4. composition to generate the letter. steps 1~3 may serve as an iteration. for example, we have a first wave of data such as @JobSubmission.html and @Internship Roster.md, and the user runs step 1~3 altogether to generate a mapping. perhaps the user reads the mapping from step 3, and thinks that it is barely held together by reasoning, and additional data is needed to reduce reasoning into a more intuitive mapping (like how the hr people will want an intuitive mapping of personal data and job description). the user decides that the some portion (say, in step 1) of the additional data an be provided by the user, and some other portions can be done by the model (in step 2), and thus, the system does the part of the data expansion needed and waits for the user to complete the second wave of data. once confirmed completed the second wave of data, the system then performs the steps required to generate the next iteration of mapping for the user to re-evaluate. these are just my thoughts. think deeply about how a workflow for the original prd can be in an age of AI and beyond."
2024-06-21T10:25:00Z - 1.1: Created the main controller (main.py) with a menu-driven interface to act as the orchestrator for the different stages. Files created: main.py
2024-06-21T10:30:00Z - 1.2: Established the data schema location by creating the data/schemas/ directory. Files created: data/schemas/
2024-06-21T10:45:00Z - 1.3: Created symmetric schema files for personal and job data to ensure data consistency and facilitate mapping. Files created: data/schemas/personal_data_schema.json, data/schemas/job_data_schema.json
2024-06-21T11:45:00Z - 2.1: Created a versatile sanitizer module with functions to convert both HTML and PDF files to clean Markdown, and updated the module to use the correct class-based API for the 'marker' library. Files created: util/sanitizer.py. Files modified: util/sanitizer.py
2024-06-21T12:00:00Z - 2.2: Implemented the full sanitization logic in the main controller, which now automatically finds, processes, and prompts for review of both personal and job description source files. Files modified: main.py
2025-06-21T13:48:07Z - Refactor: Moved unused util/pdf_to_markdown.py to the archive folder to keep the project structure clean.
